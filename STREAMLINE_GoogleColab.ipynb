{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s405-rXWFkas"
      },
      "source": [
        "![alttext](https://github.com/UrbsLab/STREAMLINE/blob/main/docs/source/pictures/STREAMLINE_Logo_Full.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgyTC3LLFvsE"
      },
      "source": [
        "# GOOGLE COLAB NOTEBOOK README\n",
        "STREAMLINE is an end-to-end automated machine learning (AutoML) pipeline that empowers anyone to easily run, interpret, and apply a rigorous and customizable analysis for data mining or predictive modeling. Currently limited to binary classification in tabular data.\n",
        "\n",
        "* This notebook runs all phases of STREAMLINE. We recommend users review the STREAMLINE documentation for details.\n",
        "\n",
        "## What to expect running this notebook 'as-is'?\n",
        "* This notebook has been initially set up to run 'as-is' on two 'demo' datasets: (1) `hcc_data.csv`: the original HCC dataset downloaded from the UCI repository and (2) `hcc_data_custom.csv`: a 'custom' datasets which removes the two covariate features from the HCC dataset, and adds simulated features and instances to it to explicitly test aspects of data preprocessing (i.e. cleaning and feature engineering).\n",
        "\n",
        "* After model training and testing evaluations are complete, the models trained from `hcc_data_custom.csv` are applied to a 'replication' dataset (`hcc_data_custom_rep.csv`) for another round of evaluations. Since no true replication data was available for this example, we simulated a replication dataset by taking the `hcc_data_custom.csv` data and randomly resampling feature values for 30% of instances in the data to add some noise/variation to it.\n",
        "\n",
        "* Notebook run parameters have initially been set up to run only three of the available modeling algorithms (logistic regression, decision tree, and naive bayes), with 3-fold CV so that it runs completely in about 6-7 minutes.\n",
        "\n",
        "* As the notebook run completes, the PDF summary(s) and zipped output folder will automatically downloaded to your computer. You will likely be propted to accept the download on your first run.\n",
        "\n",
        "## Run Instructions for this Notebook\n",
        "* **Demo Run:**\n",
        "  * Leave all run parameter cells (below) unchanged and choose `RunAll` under the `Runtime` tab in Colab Notebook\n",
        "  * You can optionally change non-essential run parameters within notebook code-cells below to run the demo with different settings\n",
        "\n",
        "* **Custom Dataset Run:**\n",
        "\n",
        "  * **Easy Mode:**\n",
        "    * Set (`demo_run = False`) and (`use_data_prompt = True`)\n",
        "    * Set any non-essential run parameters as desired within the code cells.\n",
        "    * Choose `RunAll` under `Runtime` tab in Colab Notebook.\n",
        "    * When notebook runs, the user will first be prompted to specify essential run parameters. This includes:\n",
        "      * (Required) The output folder name for the current STREAMLINE experiment\n",
        "      * (Required) Selecting dataset(s) for STREAMLINE analysis\n",
        "      * Specifying (without single or double quotation marks):\n",
        "        * (Required) class column label\n",
        "        * (If present) instanceID column label, otherwise type (`None`)\n",
        "        * (If present) Match column label (for covariate matched data), otherwise type (`None`)\n",
        "      * (Required) If replication data is available\n",
        "        * (If available) Selecting dataset(s) for replication evaluation\n",
        "        * (If available) Specifying the name of the target dataset (including file extension) whos models the replication data will be evaluating\n",
        "\n",
        "  * **Manual Mode:**\n",
        "    * Set (`demo_run = False`) and (`use_data_prompt = False`)\n",
        "    * Create a folder in /content/ (i.e. locally in the google colab workspace) that includes one or more target datasets to analyze in a single STREAMLINE experiment. Click the `Files` tab on the left to open the workspace.\n",
        "    * If you have replication data for a given target dataset, create another folder in `/content/` that includes one or more replication datasets.\n",
        "    * Edit all essential and non-essential run parameters in the code-cells accordingly.\n",
        "    * Choose `RunAll` under `Runtime` tab in Colab Notebook.\n",
        "* Before running the STREAMLINE notebook again we generally recommend selecting `Disconnect and delete runtime` under `Runtime` tab in Colab Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-5hwPRCYDZM"
      },
      "source": [
        "--------------\n",
        "# STREAMLINE RUN PARAMETERS\n",
        "--------------\n",
        "## Essential Run Parameters\n",
        "* Run parameters under 'Notebook', 'Target Data', and 'Replication Data' are typically necessary to adjust in order to run STREAMLINE new (non-demo) data\n",
        "\n",
        "* Additional run parameters below these sections can also be optionally updated to change how STREAMLINE runs, e.g. how many CV partitions to make, or how many modeling algorithms to apply\n",
        "\n",
        "### Notebook - Run Parameters\n",
        "The first two parameters below are specific to this Google Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JhBZoMAWbDaR"
      },
      "outputs": [],
      "source": [
        "demo_run = False # leave (True) to run the demo datasets, make (False) to load your own dataset folder(s) into the Google Collab workspace\n",
        "use_data_prompt = True # generally leave as (True) unless you want to dissable the prompts (that allows users to run their own data without changing the dataset run parameters in this notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JaK2AXyoSyN"
      },
      "source": [
        "### Target Data - Run Parameters (Phase 1)\n",
        "* No need to edit unless (`demo_run = False`) and (`use_data_prompt = False`)\n",
        "* Update these parameters to run STREAMLINE on a different folder of datasets. Any folder of datasets to be analyzed should include one or more datasets saved as `.txt`, `.csv` or `.tsv` files. See documentation for dataset formatting requirements.\n",
        "\n",
        "* All datasets should have the same header names for the class, instance, and match labels (note instance and match labels are optional)\n",
        "* When specifying features to be treated as categorical vs. quantitative:\n",
        "  * If these lists are left empty, STREAMLINE will assign feature type based on the `categorical_cutoff` (under 'General Run Parameters'). I.e. features with more than `categorical_cutoff` unique values will be treated as quanatiative\n",
        "  * Any binary features need not be specified as they will be assigned as categorical by default\n",
        "  * User can specify either just categorical or quanatiative feature names, and all other unspecified non-binary features will be assigned to the other feature type by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FyViM3Q-oTFU"
      },
      "outputs": [],
      "source": [
        "if not demo_run: # Leave this command as is.\n",
        "\n",
        "    # Folder path to the folder containing dataset(s) to be analyzed (must include one or more .txt, .tsv, or .csv datasets)\n",
        "    data_path = \"/content/UserData\" # (str) data folder path\n",
        "\n",
        "    # Folder path: where to save pipeline outputs (must be updated for a given user)\n",
        "    output_path = '/content/UserOutput' # (str) ouput folder path (folder will be created by STREAMLINE automatically)\n",
        "\n",
        "    # Unique experiment name - folder created for this analysis within output folder path\n",
        "    experiment_name = 'my_experiment'  # (str) experiment name (change to save a new STREAMLINE run output folder instead of overwriting previous run)\n",
        "\n",
        "    # Data Labels\n",
        "    class_label = 'Class' # (str) i.e. class outcome column name\n",
        "    instance_label = 'InstanceID' # (str) if data includes instance labels, given respective column name here, otherwise put 'None'\n",
        "    match_label = None # (str or None) only applies when M selected for partition-method; indicates column name including matched instance ids'\n",
        "\n",
        "    # Option to manually specify feature names to leave out of analysis, or which to treat as categorical vs. quantitative (without using built in variable type detector)\n",
        "    ignore_features = None # (list of str values or None) list of column names (given as string values) to exclude from the analysis (only insert column names if needed, otherwise specify 'None')\n",
        "    categorical_feature_headers = None # (list of str values or None) specify 'None' for 'auto-detect' otherwise list feature names (given as string values) to be treated as categorical.\n",
        "    quantitiative_feature_headers = None # (list of str values or None) specify 'None' for 'auto-detect' otherwise list feature names (given as string values) to be treated as quantitative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TUI02jpqAYW"
      },
      "source": [
        "### Replication Data - Run Parameters (Phase 8)\n",
        "* Don't edit unless (`demo_run = False`) and (`use_data_prompt = False`)\n",
        "\n",
        "* Update these parameters to run STREAMLINE's 'replication' phase where all models trained in the earlier phases are evaluated on the same hold-out replication data (recommended when available)\n",
        "\n",
        "* Multiple replication datasets (e.g. data collected from different sites) can be included in the replication data folder and STREAMLINE will apply replication analyses to each individually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WU5Qg99kqArq"
      },
      "outputs": [],
      "source": [
        "if not demo_run: # Leave this command as is.\n",
        "\n",
        "   # Turns the replication data analysis phase on or off\n",
        "    applyToReplication = True # (bool, True or False) leave false unless you have one or more replication datasets to further evaluate/compare all models in uniform manner\n",
        "\n",
        "    # Folder path to the folder containing the replication dataset(s) to be evaluated using previously trained models for a specific target dataset (.txt, .tsv, or .csv datasets))\n",
        "    rep_data_path = \"/content/UserRepData\" # (txt) data folder path for replication Dataset(s)\n",
        "\n",
        "    # File path to one of the individual datasets used to train models within STREAMLINE\n",
        "    dataset_for_rep = \"/content/UserData/user_data_rep.csv\" # (txt) path and name of an individual dataset used to generate the models being evaluated with replication data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lfjpdkrcGf5"
      },
      "source": [
        "--------------\n",
        "## Non-Essential Run Parameters\n",
        "### General - Run Parameters (Phase 1)\n",
        "* Optionally update these general parameters used throughout all/most phases of the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KtyZbhKUXyjF"
      },
      "outputs": [],
      "source": [
        "# Cross Validation (CV)\n",
        "n_splits = 3  # (int, > 1) number of training/testing data partitions to create - and resulting number of models generated using each ML algorithm\n",
        "partition_method = 'Stratified' # (str) with options; Stratified, Random, or Group\n",
        "\n",
        "# Cutoffs\n",
        "categorical_cutoff = 10 # (int) number of unique values after which a variable is considered to be quantitative vs categorical if categorical_features_headers or quantitative_feature_headers not specified.\n",
        "sig_cutoff = 0.05 # (float, 0-1) significance cutoff used throughout pipeline\n",
        "\n",
        "# Set Random Seed for Reproducible Analysis\n",
        "random_state = 42 # (int) sets a specific random seed for reproducible results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Pbh38acLlj"
      },
      "source": [
        "### Data Processing - Run Parameters (Phase 1)\n",
        "* Optionally, update these parameters to decide what analyses are run and outputs are produced by STREAMLINE in the exploratory analysis phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mBoYcHI0cgEz"
      },
      "outputs": [],
      "source": [
        "# EDA outpute file controls (None, outputs all files)\n",
        "exclude_eda_output = None # (None, or a list of 'str' values) with possible exclusions: ['describe','univariate_plots','correlation_plots']\n",
        "top_uni_features = 20 # (int) number of top significant features to report in notebook for univariate analysis\n",
        "\n",
        "# Data processing parameters (cleaning and feature engineering)\n",
        "featureeng_missingness = 0.5 # (float, 0-1) proportion of missing values above which categorical feature encoding missingness is generated\n",
        "cleaning_missingness = 0.5 # (float, 0-1) proportion of missing values at which instance and feature removal is performed\n",
        "correlation_removal_threshold = 1 # (float, 0-1) feature correlation at which one out of a pair of features is randomly removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBQP30iGpV6j"
      },
      "source": [
        "### Scaling and Imputing - Run Parameters (Phase 2)\n",
        "* Optionally update these parameters to turn specific data preprocessing options on or off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2hKlwP7wpWUA"
      },
      "outputs": [],
      "source": [
        "# Data Transformation (i.e. scaling) - important for running and interpreting built-in feature importance estimates for certain ML modeling algorithms\n",
        "scale_data = True # (bool, True or False) perform data scaling (recommended True)\n",
        "\n",
        "# Missing Data Imputation Options\n",
        "impute_data = True # (bool, True or False) perform missing value data imputation (required for most ML algorithms if missing data is present)\n",
        "multi_impute = True # (bool, True or False) apply multivariate imputation to quantitative features, otherwise uses mean imputation\n",
        "\n",
        "# When False, optionally keep the intermediary CV files generated in Phase 1, otherwise overwrite them with new scaled/imputed ones\n",
        "overwrite_cv = False # (bool, True or False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20i7JqJH332M"
      },
      "source": [
        "### Feature Importance Estimation - Run Parameters (Phase 3)\n",
        "* Optionally update these parameters to decide which filter-based feature importance estimation algorithms to apply (currently only mutual information and MultiSURF are options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CtQxdYfi34P8"
      },
      "outputs": [],
      "source": [
        "# Available Filter-based Feature Importance/Selection Algorithms\n",
        "do_mutual_info = True # (bool, True or False) do mutual information analysis\n",
        "do_multisurf = True # (bool, True or False) do multiSURF analysis\n",
        "\n",
        "# Additional MultiSURF Options\n",
        "use_TURF = False # (bool, True or False) use TURF wrapper around MultiSURF (recommended for datasets with >10,000 features)\n",
        "TURF_pct = 0.5 # (float, 0.01-0.5) proportion of instances removed in an iteration (also dictates number of iterations as 1/TURF_pct)\n",
        "instance_subset = 2000 # (int) sample subset size to use with MultiSURF (since MultiSURF's compute time scales quadratically with instance count)\n",
        "njobs = -1 # (int) number of cores dedicated to running algorithm; setting to -1 will use all available cores when run locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXlPgkcn6T0A"
      },
      "source": [
        "### Feature Selection - Run Parameters (Phase 4)\n",
        "* Optionally update these parameters to control how 'collective' feature selection is conducted prior to modeling.\n",
        "\n",
        "  * When `filter_poor_features = False`, all features will be used in the modeling phase.\n",
        "\n",
        "  * When `filter_poor_features = True`:\n",
        "    * And `max_features_to_keep = None`, all features with a score <= 0 from all active feature importance algorithm will be removed, but the rest kept.\n",
        "\n",
        "    * And `max_features_to_keep = n` (where n is a 'value' less than the total number of features in the dataset), first all features with a score <= 0 from all active feature importance algorithm will be removed, then the top n scoring (non-redundant) features from each algorithm will be kept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9I5eaBmU6UEl"
      },
      "outputs": [],
      "source": [
        "# Turn feature selection on or off\n",
        "filter_poor_features = True # (bool, True or False) filter out features with no indication of being informative prior to modeling\n",
        "\n",
        "# Control maximum number of features to keep out of total features in dataset.\n",
        "max_features_to_keep = 2000 # (int or None) maximum features to keep. 'None' if no max\n",
        "\n",
        "# Controls the feature importance estimation plots generation\n",
        "top_fi_features = 40 # (int) number of top scoring features to illustrate in feature importance figures\n",
        "export_scores = True # (bool, True or False) export figure summarizing average feature importance scores over cv partitions\n",
        "\n",
        "# When False, optionally keep the intermediary CV files generated in Phase 2, otherwise overwrite them with new feature selected ones\n",
        "overwrite_cv_feat = False # (bool, True or False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmjcXp6y9yDC"
      },
      "source": [
        "### Modeling - Run Parameters (Phase 5)\n",
        "* Optionally update these parameters to control what modeling algorithms are run, as well as other options relevant to the modeling phase. The 16 Classification algorithms currently available in STREAMLINE include:\n",
        "\n",
        "  * Naive Bayes (NB)\n",
        "  * Logistic Regression (LR)\n",
        "  * Elastic Net (EN)\n",
        "  * Decision Tree (DT)\n",
        "  * Random Forest (RF)\n",
        "  * Gradient Boosting (GB)\n",
        "  * Extreame Gradient Boosting (XGB)\n",
        "  * Light Gradient Boosting (LGB)\n",
        "  * Category Gradient Boosting (CGB)\n",
        "  * Support Vector Machines (SVM)\n",
        "  * Artificial Neural Networks (ANN)\n",
        "  * K-Nearest Neighbors (KNN)\n",
        "  * Genetic Programming, i.e. symbolic classification (GP)\n",
        "  * Educational Learning Classifier System (eLCS)\n",
        "  * 'X' Classifier System (XCS)\n",
        "  * Extended Supervised Tracking Classifier System (ExSTraCS)\n",
        "\n",
        "The last 3 algorithms above are rule-based ML approaches implemented by our research group. eLCS and XCS are under active development so they have been turned off when using default settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tc_cuqYo9yTF"
      },
      "outputs": [],
      "source": [
        "# Machine Learning Algorithms to Run (Setting 'algorithms' to 'None' rather than a list will run all algorithms except those specified in 'exclude')\n",
        "algorithms = [\"NB\", \"LR\", \"DT\"] # (list of strings or None) options: [\"NB\",\"LR\",\"EN\",\"DT\",\"RF\",\"GB\",\"XGB\",\"LGB\",\"CGB\",\"SVM\",\"ANN\",\"KNN\",\"GP\",\"eLCS\",\"XCS\",\"ExSTraCS\"]\n",
        "\n",
        "# ML Model Algorithm to exclude (no need to fill out if 'algorithms' are specified)\n",
        "exclude = ['eLCS', 'XCS'] # (list of strings or None) options: [\"NB\",\"LR\",\"EN\",\"DT\",\"RF\",\"GB\",\"XGB\",\"LGB\",\"CGB\",\"SVM\",\"ANN\",\"KNN\",\"GP\",\"eLCS\",\"XCS\",\"ExSTraCS\"]\n",
        "\n",
        "# Other Analysis Parameters\n",
        "training_subsample = 0  # (int) for long running algorithms, option to subsample training set (0 for no subsample) to limit the sample size used to train algorithms that do not scale up well in large instance spaces (i.e. XGB,SVM,KN,ANN,and LR) and depending on 'instances' settings, ExSTraCS, eLCS, and XCS)\n",
        "use_uniform_FI = True # (bool, True or False) overides use of any available feature importances estimate methods from models, instead using permutation_importance uniformly\n",
        "primary_metric = 'balanced_accuracy' # (str) metric used to optimize hyperparameters: must be an available metric identifier from (https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
        "metric_direction = 'maximize' # (str) options 'maximize' or 'minimize': must be selected appropriately for the chosen primary metric (generally maximize)\n",
        "\n",
        "# Hyperparameter Sweep Options\n",
        "n_trials = 200   # (int) number of bayesian hyperparameter optimization trials using Optuna\n",
        "timeout = 900    # (int or None) seconds until hyperparameter sweep stops running new trials (Note: it may run longer to finish last trial started): must be set to None to ensure STREAMLINE reproducibility\n",
        "export_hyper_sweep_plots = True # (bool, True or False) export hyperparameter sweep plots generated with Optuna\n",
        "\n",
        "# Learning classifier system algorithm options (ExSTraCS, eLCS, XCS)\n",
        "do_lcs_sweep = False # (bool, True or False) do LCS hyperparameter tuning otherwise use specified hyperparameter settings below (we recommend leaving this False, as it can take a long time to run)\n",
        "lcs_nu = 1                 # (int, 0-10) specify LCS nu parameter (higher values place more pressure to generate accurate rules, but easily leads to overfitting in noisy problems)\n",
        "lcs_iterations = 200000    # (int, > training data instance count) specify the number of LCS learning iterations to conduct\n",
        "lcs_N = 2000               # (int) > 500) specify the maximum rule population size for the LCS algorithm\n",
        "lcs_timeout = 1200         # (int) seconds until hyperparameter sweep stops for LCS algorithms (note: evolutionary algorithms often require more time for a single run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zKal9-MB1IH"
      },
      "source": [
        "### Post-Analysis - Run Parameters (Phase 6)\n",
        "* Optionally update these parameters to control aspects of model evaluation figure generation. Note that all STREAMLINE performance metric evaluations and figures are generated with respect to the hold out testing data in this phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9ZoJ7JVIB1aP"
      },
      "outputs": [],
      "source": [
        "# Post-analysis output file controls\n",
        "exclude_plots = None # (None, or a list of 'str' values) with possible exclusions: ['plot_ROC', 'plot_PRC', 'plot_FI_box', 'plot_metric_boxplots']\n",
        "top_model_fi_features = 40  # (int) number of top features in model to illustrate in feature importance figures\n",
        "metric_weight = 'balanced_accuracy' # (str, balanced_accuracy or roc_auc) ML model metric used as weight in composite FI plots (only supports balanced_accuracy or roc_auc as options): recommend setting the same as primary_metric if possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8gmfGNEpUz1"
      },
      "source": [
        "### Replication - Run Parameters (Phase 8)\n",
        "* Optionally update these parameters if you're running replication phase to exclude plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eqs3cpaLpZLL"
      },
      "outputs": [],
      "source": [
        "# Replication output file controls\n",
        "exclude_rep_plots = None # (None, outputs all files) with possible exlusions ['plot_ROC', 'plot_PRC', 'plot_metric_boxplots','feature_correlations']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZHO8BTccGuR"
      },
      "source": [
        "### Cleanup - Run Parameters\n",
        "* Optionally update these parameters to delete temporary files in output folder (generally recommended to leave these as `True`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aWhgkZFTcG-H"
      },
      "outputs": [],
      "source": [
        "del_time = False  # (bool, True or False) delete individual run-time files (but save summary)\n",
        "del_old_cv = False # (bool, True or False) delete any of the older versions of CV training and testing datasets if overwrite_cv was set to False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGjuHXOdZFyh"
      },
      "source": [
        "-------------\n",
        "\n",
        "# Most Users - We recommend you do not make code edits below this cell.\n",
        "\n",
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDP9cvFQTN1N"
      },
      "source": [
        "# DEMO DATA ANALYSIS SETUP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej-ZhNo_ZKzX"
      },
      "source": [
        "### Demo Data Run Parameters:\n",
        "* Set up for Demo Run (never edit the code cell below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "G46uzad2RJPL"
      },
      "outputs": [],
      "source": [
        "if demo_run: # Leave this command as is.\n",
        "\n",
        "    # Folder path to the folder containing dataset(s) to be analyzed (must include one or more .txt, .tsv, or .csv datasets)\n",
        "    data_path = \"/content/STREAMLINE/data/DemoData\" # (str) data folder path\n",
        "\n",
        "    # Output foder path: where to save pipeline outputs (must be updated for a given user)\n",
        "    output_path = '/content/DemoOutput' # (str) ouput folder path (folder will be created by STREAMLINE automatically)\n",
        "\n",
        "    # Unique experiment name - folder created for this analysis within output folder path\n",
        "    experiment_name = 'demo_experiment'  # (str) experiment name (change to save a new STREAMLINE run output folder instead of overwriting previous run)\n",
        "\n",
        "    # Data Labels\n",
        "    class_label = 'Class' # (str) i.e. class outcome column name\n",
        "    instance_label = 'InstanceID' # (str) If data includes instance labels, given respective column name here, otherwise put 'None'\n",
        "    match_label = None # (str or None) only applies when M selected for partition-method; indicates column name including matched instance ids'\n",
        "\n",
        "    # Option to manually specify feature names to leave out of analysis\n",
        "    ignore_features = None # list of column names (given as string values) to exclude from the analysis (only insert column names if needed, otherwise leave empty)\n",
        "\n",
        "    # Recommended option to manually specify what features to treat as categorical: None for 'auto-detect', otherwise list feature names (given as string values) to be treated as categorical\n",
        "    categorical_feature_headers = ['Gender','Symptoms','Alcohol','Hepatitis B Surface Antigen','Hepatitis B e Antigen','Hepatitis B Core Antibody','Hepatitis C Virus Antibody','Cirrhosis',\n",
        "                                   'Endemic Countries','Smoking','Diabetes','Obesity','Hemochromatosis','Arterial Hypertension','Chronic Renal Insufficiency','Human Immunodeficiency Virus',\n",
        "                                   'Nonalcoholic Steatohepatitis','Esophageal Varices','Splenomegaly','Portal Hypertension','Portal Vein Thrombosis','Liver Metastasis','Radiological Hallmark',\n",
        "                                   'Sim_Cat_2','Sim_Cat_3','Sim_Cat_4','Sim_Text_Cat_2','Sim_Text_Cat_3','Sim_Text_Cat_4']\n",
        "\n",
        "    # Recommended option to manually specify what features to treat as quantitative: None for 'auto-detect', otherwise list feature names (given as string values) to be treated as quantitative\n",
        "    quantitiative_feature_headers = ['Age at diagnosis','Grams of Alcohol per day','Packs of cigarets per year', 'Performance Status*', 'Encephalopathy degree*','Ascites degree*',\n",
        "                                      'International Normalised Ratio*','Alpha-Fetoprotein (ng/mL)','Haemoglobin (g/dL)','Mean Corpuscular Volume', 'Leukocytes(G/L)',\n",
        "                                      'Platelets','Albumin (mg/dL)','Total Bilirubin(mg/dL)','Alanine transaminase (U/L)','Aspartate transaminase (U/L)','Gamma glutamyl transferase (U/L)',\n",
        "                                      'Alkaline phosphatase (U/L)', 'Total Proteins (g/dL)', 'Creatinine (mg/dL)','Number of Nodules','Major dimension of nodule (cm)','Direct Bilirubin (mg/dL)',\n",
        "                                      'Iron','Oxygen Saturation (%)','Ferritin (ng/mL)','Sim_Miss_0.6','Sim_Miss_0.7','Sim_Cor_-1.0_A','Sim_Cor_-1.0_B','Sim_Cor_0.9_A', 'Sim_Cor_0.9_B',\n",
        "                                      'Sim_Cor_1.0_A','Sim_Cor_1.0_B']\n",
        "\n",
        "    # Turns the replication data analysis phase on or off\n",
        "    applyToReplication = True # (bool, True or False) leave false unless you have a replication dataset handy to further evaluate/compare all models in uniform manner\n",
        "\n",
        "    # Folder path to the folder containing the replication dataset(s) to be evaluated using previously trained models (.txt, .tsv, or .csv datasets))\n",
        "    rep_data_path = \"/content/STREAMLINE/data/DemoRepData\" # (txt) name of folder with replication dataset(s)\n",
        "\n",
        "    # File path to one of the individual datasets used to train models within STREAMLINE\n",
        "    dataset_for_rep = \"/content/STREAMLINE/data/DemoData/hcc_data_custom.csv\" # (txt) path and name of an individual dataset used to generate the models being evaluated with replication data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV_tzea2hnFY"
      },
      "source": [
        "-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcJt-Gc4go8A"
      },
      "source": [
        "# PROMPT SETUP FOR DATASET PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rq9TndZzTVWE"
      },
      "outputs": [],
      "source": [
        "def run_prompts(demo_run,use_data_prompt,original_wd):\n",
        "    import os\n",
        "\n",
        "    output_path = '/content/UserOutput' # Make separate ouput folder to save STREAMLINE experiment runs on non-demo data\n",
        "\n",
        "    # Get and save user datasets locally\n",
        "    custom_data_path = '/content/UserData' # Make local folder for saving user specified target datasets for STREAMLINE anlaysis\n",
        "\n",
        "    # Check if the directory exists\n",
        "    if not os.path.exists(custom_data_path):\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(custom_data_path)\n",
        "    else:\n",
        "        # Traverse the directory tree and delete all files and directories\n",
        "        for root, dirs, files in os.walk(custom_data_path, topdown=False):\n",
        "            for name in files:\n",
        "                os.remove(os.path.join(root, name))\n",
        "            for name in dirs:\n",
        "                os.rmdir(os.path.join(root, name))\n",
        "        os.rmdir(custom_data_path)\n",
        "        os.makedirs(custom_data_path)\n",
        "\n",
        "    os.chdir(custom_data_path)\n",
        "\n",
        "    # Ask user for unique experiment name (we do this first so the screen jumps to this prompt)\n",
        "    experiment_name = input(\"Enter unique experiment name for output folder (required, no spaces): \\n\")\n",
        "\n",
        "    # Have user upload target dataset(s)\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload() # Prompt user to select one or more datasets to upload into the 'UserData' folder\n",
        "    os.chdir(original_wd)\n",
        "\n",
        "    # Gather other necessary target data information\n",
        "    class_label = input(\"Enter header label of class column (required): \\n\")\n",
        "    instance_label = input(\"Enter header label of instance ID or specify None: \\n\")\n",
        "    match_label = input(\"Enter header label of match column or specify None: \\n\")\n",
        "    if instance_label == \"None\":\n",
        "        instance_label = None\n",
        "    if match_label == \"None\":\n",
        "        match_label = None\n",
        "\n",
        "    # Ask user whether replication dataset(s) are available\n",
        "    applyToReplication = eval(input(\"Is replication data available? Enter True or False (required): \\n\"))\n",
        "    custom_rep_data_path = None\n",
        "    dataset_for_rep = None\n",
        "    if applyToReplication:\n",
        "        custom_rep_data_path = '/content/UserRepData' # Make local folder for saving user specified target datasets for STREAMLINE anlaysis\n",
        "\n",
        "        # Check if the directory exists\n",
        "        if not os.path.exists(custom_rep_data_path):\n",
        "            # Create the directory if it doesn't exist\n",
        "            os.makedirs(custom_rep_data_path)\n",
        "        else:\n",
        "            # Traverse the directory tree and delete all files and directories\n",
        "            for root, dirs, files in os.walk(custom_rep_data_path, topdown=False):\n",
        "                for name in files:\n",
        "                    os.remove(os.path.join(root, name))\n",
        "                for name in dirs:\n",
        "                    os.rmdir(os.path.join(root, name))\n",
        "            os.rmdir(custom_rep_data_path)\n",
        "            os.makedirs(custom_rep_data_path)\n",
        "\n",
        "        os.chdir(custom_rep_data_path)\n",
        "\n",
        "        # Have user upload replication dataset(s)\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload() # Prompt user to select one or more datasets to upload into the 'UserData' folder\n",
        "        os.chdir(original_wd)\n",
        "\n",
        "        #Have user idenify the name of the target dataset used to train the model to which the replication dataset(s) will be applied\n",
        "        dataset_for_rep = custom_data_path +'/'+ input('Enter the filename (with extension) of the dataset in /UserData/ to indicate which models the replication data will be applied to: \\n')\n",
        "\n",
        "    return custom_data_path,custom_rep_data_path,class_label,instance_label,match_label,dataset_for_rep,applyToReplication,experiment_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "I4PLWbQsUZ0k",
        "outputId": "ea44340d-7c30-47b1-971a-d04d0aba0948"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\nitin\\Desktop\\Internships\\KidneyCancerML\\STREAMLINE_GoogleColab.ipynb Cell 34\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m original_wd \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetcwd()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m demo_run \u001b[39mand\u001b[39;00m use_data_prompt:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Run User Prompts\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     data_path,rep_data_path,class_label,instance_label,match_label,dataset_for_rep,applyToReplication,experiment_name \u001b[39m=\u001b[39m run_prompts(demo_run,use_data_prompt,original_wd)\n",
            "\u001b[1;32mc:\\Users\\nitin\\Desktop\\Internships\\KidneyCancerML\\STREAMLINE_GoogleColab.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m experiment_name \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter unique experiment name for output folder (required, no spaces): \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Have user upload target dataset(s)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m uploaded \u001b[39m=\u001b[39m files\u001b[39m.\u001b[39mupload() \u001b[39m# Prompt user to select one or more datasets to upload into the 'UserData' folder\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nitin/Desktop/Internships/KidneyCancerML/STREAMLINE_GoogleColab.ipynb#X45sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m os\u001b[39m.\u001b[39mchdir(original_wd)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Get current working directory\n",
        "original_wd = os.getcwd()\n",
        "\n",
        "if not demo_run and use_data_prompt:\n",
        "    # Run User Prompts\n",
        "    data_path,rep_data_path,class_label,instance_label,match_label,dataset_for_rep,applyToReplication,experiment_name = run_prompts(demo_run,use_data_prompt,original_wd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BopjHVZnVEM3"
      },
      "outputs": [],
      "source": [
        "# Leave this empty code cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgqLm-qZhHc7"
      },
      "source": [
        "# STREAMLINE RUN CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42TpOz1eF1R9"
      },
      "source": [
        "## Install STREAMLINE and Prerequisites\n",
        "* Downloads most recent version of STREAMLINE from GitHub and installs other packages required by STREAMLINE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJXUp8_yFrRn",
        "outputId": "753ea6f7-9ae3-44e6-e7de-dcf4c6b80d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'STREAMLINE' already exists and is not an empty directory.\n",
            "/content/STREAMLINE\n"
          ]
        }
      ],
      "source": [
        "!git clone -b main https://github.com/UrbsLab/STREAMLINE.git -q\n",
        "#!git clone -b dev https://github.com/UrbsLab/STREAMLINE.git -q\n",
        "\n",
        "%cd STREAMLINE\n",
        "\n",
        "!pip install -r requirements.txt &> /dev/null\n",
        "!pip install --upgrade scipy>=1.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cbakQ8iF_kl"
      },
      "source": [
        "## Notebook Housekeeping\n",
        "* Sets up notebook cells to display internal process\n",
        "\n",
        "* Use `logging.INFO` for higher level output, `logging.WARNING` for only critical information. Comment to hide all text output.\n",
        "\n",
        "* You can use `run_parallel = True` for phases other than modeling, but the advantage is not significant vs the overhead for small jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0Y-6f2x85co"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "FORMAT = '%(levelname)s: %(message)s'\n",
        "logging.basicConfig(format=FORMAT)\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP4W9tfhRHmd"
      },
      "source": [
        "* Housekeeping code allowing notebook to be run again with the same settings, overwriting a previously run experiment with the same name\n",
        "  * Comment out code cell below to avoid this behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMK0OPphGHGZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "if os.path.exists(output_path+'/'+experiment_name):\n",
        "    shutil.rmtree(output_path+'/'+experiment_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbkz75LgGD9r"
      },
      "source": [
        "## STREAMLINE Workflow\n",
        "* The code below runs through the analysis phases of STREAMLINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-k3iBM190S6"
      },
      "source": [
        "## Phase 1: Data Processing Phase\n",
        "After cell runs, for each target dataset you will see:\n",
        "* An initial EDA data counts summary\n",
        "* Notification of features removed (during cleaning) or added (during feature engineering)\n",
        "* A processed EDA data counts summary\n",
        "* Class balance barplot\n",
        "* Feature correlation heatmap\n",
        "* Top univariate analysis results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofRidh1S9xgE",
        "outputId": "a55ac37a-bf7c-4b53-a0ef-98501b2de837"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:------------------------------------------------------- \n",
            "INFO:root:Loading Dataset: data_preprocessed_with_label+y\n",
            "WARNING:root:Warning: Specified 'Match label' could not be found in dataset. Analysis moving forward assuming there is no 'match label' column using stratified (S) CV partitioning.\n",
            "INFO:root:Validating and Identifying Feature Types...\n",
            "WARNING:root:User did not specify categorical vs quantitative features; feature types will be automatically assigned based on categorical_cutoff parameter\n",
            "INFO:root:Running Initial EDA:\n",
            "INFO:root:Initial Data Counts: ----------------\n",
            "INFO:root:Instance Count = 533\n",
            "INFO:root:Feature Count = 20532\n",
            "INFO:root:    Categorical  = 531\n",
            "INFO:root:    Quantitative = 20001\n",
            "INFO:root:Missing Count = 0\n",
            "INFO:root:    Missing Percent = 0.0\n",
            "INFO:root:Class Counts: ----------------\n",
            "INFO:root:Class Count Information\n",
            "INFO:root:\n",
            "   Class  Instances\n",
            "0      0        456\n",
            "1      1         77\n"
          ]
        }
      ],
      "source": [
        "from streamline.runners.dataprocess_runner import DataProcessRunner\n",
        "dpr = DataProcessRunner(data_path, output_path, experiment_name,\n",
        "                exclude_eda_output=exclude_eda_output,\n",
        "                class_label=class_label, instance_label=instance_label,\n",
        "                match_label=match_label, n_splits=n_splits,\n",
        "                partition_method=partition_method,\n",
        "                ignore_features=ignore_features,\n",
        "                categorical_features=categorical_feature_headers,\n",
        "                quantitative_features=quantitiative_feature_headers,\n",
        "                top_features=top_uni_features,\n",
        "                categorical_cutoff=categorical_cutoff, sig_cutoff=sig_cutoff,\n",
        "                featureeng_missingness=featureeng_missingness,\n",
        "                cleaning_missingness=cleaning_missingness,\n",
        "                correlation_removal_threshold=correlation_removal_threshold,\n",
        "                random_state=random_state, show_plots=True)\n",
        "dpr.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJBtvpO-CxMU"
      },
      "source": [
        "## Phase 2: Scaling and Imputation\n",
        "After cell runs, you will see:\n",
        "* No output other than code progress updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRPCoPEG-FZD"
      },
      "outputs": [],
      "source": [
        "from streamline.runners.imputation_runner import ImputationRunner\n",
        "ir = ImputationRunner(output_path, experiment_name,\n",
        "                        scale_data=scale_data, impute_data=impute_data,\n",
        "                        multi_impute=multi_impute, overwrite_cv=overwrite_cv,\n",
        "                        class_label=class_label, instance_label=instance_label,\n",
        "                        random_state=random_state)\n",
        "ir.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuAxzygTETa2"
      },
      "source": [
        "## Phase 3: Feature Importance Evaluation\n",
        "After cell runs, you will see:\n",
        "* No output other than code progress updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EF0mLemYKom"
      },
      "outputs": [],
      "source": [
        "feat_algorithms = []\n",
        "if do_mutual_info:\n",
        "    feat_algorithms.append(\"MI\")\n",
        "if do_multisurf:\n",
        "    feat_algorithms.append(\"MS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1X2jWFXETAw"
      },
      "outputs": [],
      "source": [
        "from streamline.runners.feature_runner import FeatureImportanceRunner\n",
        "f_imp = FeatureImportanceRunner(output_path, experiment_name,\n",
        "                                class_label=class_label,\n",
        "                                instance_label=instance_label,\n",
        "                                instance_subset=instance_subset,\n",
        "                                algorithms=feat_algorithms,\n",
        "                                use_turf=use_TURF, turf_pct=TURF_pct,\n",
        "                                random_state=random_state)\n",
        "f_imp.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2udkSXOYEx21"
      },
      "source": [
        "## Phase 4: Feature Selection\n",
        "After cell runs, for each target dataset and each feature importance algorithm you will see:\n",
        "* Top feature importance scores\n",
        "* A barplot of top feature imporance score ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nip62hw-EZ5K"
      },
      "outputs": [],
      "source": [
        "from streamline.runners.feature_runner import FeatureSelectionRunner\n",
        "f_sel = FeatureSelectionRunner(output_path, experiment_name,\n",
        "                               feat_algorithms, class_label=class_label,\n",
        "                               instance_label=instance_label,\n",
        "                               max_features_to_keep=max_features_to_keep,\n",
        "                               filter_poor_features=filter_poor_features,\n",
        "                               top_features=top_fi_features,\n",
        "                               export_scores=export_scores,\n",
        "                               overwrite_cv=overwrite_cv_feat,\n",
        "                               random_state=random_state,\n",
        "                               show_plots=True)\n",
        "f_sel.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e9Bk0SxFPIZ"
      },
      "source": [
        "## Phase 5: Modeling\n",
        "After cell runs, you will see:\n",
        "* No output other than code progress bar completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hOuYSGfE5jB"
      },
      "outputs": [],
      "source": [
        "from streamline.runners.model_runner import ModelExperimentRunner\n",
        "model_exp = ModelExperimentRunner(\n",
        "                output_path, experiment_name, algorithms=algorithms,\n",
        "                exclude=exclude, class_label=class_label,\n",
        "                instance_label=instance_label, scoring_metric=primary_metric,\n",
        "                metric_direction=metric_direction,\n",
        "                training_subsample=training_subsample,\n",
        "                use_uniform_fi=use_uniform_FI, n_trials=n_trials,\n",
        "                timeout=timeout, save_plots=False,\n",
        "                do_lcs_sweep=do_lcs_sweep, lcs_nu=lcs_nu, lcs_n=lcs_N,\n",
        "                lcs_iterations=lcs_iterations,\n",
        "                lcs_timeout=lcs_timeout, resubmit=False)\n",
        "model_exp.run(run_parallel=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0sJWBScIDc4"
      },
      "source": [
        "## Phase 6: Statistics Summary and Figure Generation\n",
        "After cell runs, for each target dataset you will see:\n",
        "* ROC and PRC plots of CV folds for each algorithm\n",
        "* An ROC and PRC plot comparing average algorithm performance across CV partitions\n",
        "* Boxplots for each metric comparing algorithm performance (across CV partitions)\n",
        "* Top feature importance boxplots for each algorithm (across CV partitions)\n",
        "* Histogram of feature importance for each algorithm\n",
        "* Composite feature importance plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwcdh3W3IHc3"
      },
      "outputs": [],
      "source": [
        "from streamline.runners.stats_runner import StatsRunner\n",
        "stats = StatsRunner(output_path, experiment_name,\n",
        "                    algorithms=algorithms, exclude=exclude,\n",
        "                    class_label=class_label, instance_label=instance_label,\n",
        "                    scoring_metric=primary_metric,\n",
        "                    top_features=top_model_fi_features, sig_cutoff=sig_cutoff,\n",
        "                    metric_weight=metric_weight, scale_data=scale_data,\n",
        "                    exclude_plots=exclude_plots,\n",
        "                    show_plots=True)\n",
        "stats.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqfgPhzBL0Xb"
      },
      "source": [
        "## Phase 7: Dataset Comparison\n",
        "* Optional: Used only if > 1 dataset was analyzed\n",
        "\n",
        "Assuming STREAMLINE was run on more than 1 dataset. After cell runs, for each evaluation metric you will see:\n",
        "* Boxplots (for each metric) showing the distribution of median CV model performance (one data point for each algorithm) within a single sub-boxplot, run for each target dataset\n",
        "  * Lines between boxplots show how the median ML algorithm performance changed from one dataset to the next\n",
        "* Boxplots (for each algorithm and either ROC-AUC or PRC-AUC) showing the distribution of CV model performances within a single sub-boxplot, for each target dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv7O5jc3LzvG"
      },
      "outputs": [],
      "source": [
        "# Function to check target data folder for more than one dataset\n",
        "def len_datasets(output_path, experiment_name):\n",
        "    datasets = os.listdir(output_path + '/' + experiment_name)\n",
        "    remove_list = ['metadata.pickle', 'metadata.csv', 'algInfo.pickle',\n",
        "                   'jobsCompleted', 'logs', 'jobs', 'DatasetComparisons', 'UsefulNotebooks',\n",
        "                   experiment_name + '_ML_Pipeline_Report.pdf']\n",
        "    for text in remove_list:\n",
        "        if text in datasets:\n",
        "            datasets.remove(text)\n",
        "    return len(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_frEMK4KhPI"
      },
      "outputs": [],
      "source": [
        "from streamline.runners.compare_runner import CompareRunner\n",
        "if len_datasets(output_path, experiment_name) > 1:\n",
        "    cmp = CompareRunner(output_path, experiment_name, algorithms=algorithms,\n",
        "                        exclude=exclude, sig_cutoff=sig_cutoff,\n",
        "                        class_label=class_label, instance_label=instance_label,\n",
        "                        show_plots=True)\n",
        "    cmp.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTH3xMl8QchK"
      },
      "source": [
        "## Phase 8: Replication\n",
        "* Optional - depends on availability of replication data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vASWHToXSMpX"
      },
      "outputs": [],
      "source": [
        "if applyToReplication:\n",
        "    from streamline.runners.replicate_runner import ReplicationRunner\n",
        "    repl = ReplicationRunner(rep_data_path, dataset_for_rep, output_path,\n",
        "                             experiment_name, load_algo=True,\n",
        "                             exclude_plots=exclude_rep_plots)\n",
        "    repl.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaqYpZViPPVc"
      },
      "source": [
        "## Phase 9: Summary Report\n",
        "* Optional\n",
        "* Generates and downloads a PDF report of the analysis\n",
        "\n",
        "### Testing Data Report\n",
        "* Summarizes testing evaluations on all trained models, applied to their respective hold out testing data partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9GVbQOrPb2G"
      },
      "outputs": [],
      "source": [
        "from streamline.runners.report_runner import ReportRunner\n",
        "rep = ReportRunner(output_path, experiment_name,\n",
        "                   algorithms=algorithms, exclude=exclude)\n",
        "rep.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzbQBgH4RjQW"
      },
      "source": [
        "### Replication Data Report\n",
        "Summarizes performance of all trained models when applied to the same replication dataset. This evaluation offers a better way to pick a 'best performing' model, since all models are evaluated on the same set of new, or as-of-yet unseen, data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-10eE_nUioQ"
      },
      "outputs": [],
      "source": [
        "if applyToReplication:\n",
        "    from streamline.runners.report_runner import ReportRunner\n",
        "    rep = ReportRunner(output_path=output_path, experiment_name=experiment_name,\n",
        "                       algorithms=algorithms, exclude=exclude, training=False,\n",
        "                       rep_data_path=rep_data_path,\n",
        "                       dataset_for_rep=dataset_for_rep)\n",
        "    rep.run(run_parallel=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyNBJgShRk6w"
      },
      "source": [
        "## File Cleanup\n",
        "* Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpTJmdhZQR9D"
      },
      "outputs": [],
      "source": [
        "from streamline.runners.clean_runner import CleanRunner\n",
        "clean = CleanRunner(output_path, experiment_name, del_time=del_time, del_old_cv=del_old_cv)\n",
        "# run_parallel is not used in clean\n",
        "clean.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BarOZn6BLMCG"
      },
      "source": [
        "## Output\n",
        "### Download and Open PDF Summary Report(s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6pWkCpKLMQb"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(output_path + '/' + experiment_name + '/' + experiment_name + '_STREAMLINE_Report.pdf')\n",
        "\n",
        "if applyToReplication:\n",
        "    dataset_name = dataset_for_rep.split('/')[-1].split('.')[0]\n",
        "    #from google.colab import files\n",
        "    pdf_files = []\n",
        "    for dirpath, dirnames, filenames in os.walk(output_path + '/' + experiment_name\n",
        "                                                + '/' + dataset_name + '/replication/'):\n",
        "        for filename in [f for f in filenames if f.endswith(\".pdf\")]:\n",
        "            pdf_files.append(os.path.join(dirpath, filename))\n",
        "    for file_path in pdf_files:\n",
        "        files.download(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU_qUle17uul"
      },
      "source": [
        "## Zip the experiment folder and download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZoiBH7G7ZIc"
      },
      "outputs": [],
      "source": [
        "experiment_folder = output_path + '/' + experiment_name\n",
        "!zip -r -q /content/{experiment_name}.zip {experiment_folder}\n",
        "from google.colab import files\n",
        "files.download('/content/' + experiment_name + '.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsb7ISnpNFuF"
      },
      "outputs": [],
      "source": [
        "#Return notebook to original directory to avoid nested STREAMLINE download bug.\n",
        "os.chdir(original_wd)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
